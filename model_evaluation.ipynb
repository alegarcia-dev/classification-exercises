{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17519606-8e71-40df-a5fd-075e1e7011a1",
   "metadata": {},
   "source": [
    "**Model Evaluation Exercises**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99bdf22-4f9c-40ae-9e5a-dc59f94e63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2877b-8d65-4852-8562-1d36484e6b4e",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "\n",
    "We'll take a positive case to be a dog and negative case to be a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7534bb8-6101-4c8a-a94d-5b4f01edb087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = 46\n",
    "TN = 34\n",
    "FP = 13\n",
    "FN = 7\n",
    "\n",
    "# accuracy is total correct predictions over total observations.\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c7a9d0-69b1-4746-a655-cda824c28388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision is true positives over total positive predictions.\n",
    "precision = TP / (TP + TN)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00713e4-493a-4e8e-ba8e-fbdcf370a98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679245283018868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall is true positives over total actual positives.\n",
    "recall = TP / (TP + FN)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52999685-3616-4eb2-8ce7-e977e27c73e2",
   "metadata": {},
   "source": [
    "For our baseline we will predict dog everytime.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         53 |         0  |\n",
    "| actual cat    |         47 |         0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e834d4-8a86-484a-9e6e-03a5eab061bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = 53\n",
    "TN = 0\n",
    "FP = 47\n",
    "FN = 0\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bf86d-4a85-42b3-b4e1-7c62ef6982d1",
   "metadata": {},
   "source": [
    "- In the context of this problem, what is a false positive?\n",
    "\n",
    "A false positive would be predicting dog when really it was a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962647a-143b-4b4d-a347-f1105f31602e",
   "metadata": {},
   "source": [
    "- In the context of this problem, what is a false negative?\n",
    "\n",
    "A false negative would be predicting cat when really it was a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78e2e6-0ebe-4060-9e27-b3f036b64eee",
   "metadata": {},
   "source": [
    "- How would you describe this model?\n",
    "\n",
    "Since there is no particular consequence of false positive versus false negative for this model we will assess the performance based on accuracy. Compared to the baseline this model, in terms of accuracy, performs better overall making accurate predictions 80% of the time versus 53% of the time for the baseline model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef62f27-7678-49da-a1fd-37a6dacdbb8b",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20d32e5-e406-40ed-81f7-cc7a5ecf5c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3 = pd.read_csv('c3.csv')\n",
    "c3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e915b40-2874-4d80-ba59-308e524b7688",
   "metadata": {},
   "source": [
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "- An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972d0c1-2487-46ff-a043-0ae6da28e2d7",
   "metadata": {},
   "source": [
    "Given that defect is positive, we should optimize for recall since we do not want to miss positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba00022-ee79-4688-9c6b-89931222bdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5625, 0.8125]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = []\n",
    "columns = ['model1', 'model2', 'model3']\n",
    "\n",
    "for column in columns:\n",
    "    positive = c3[column] == 'Defect'\n",
    "    correct = c3.actual == c3[column]\n",
    "    tp = c3[positive & correct].shape[0]\n",
    "\n",
    "    negative = c3[column] == 'No Defect'\n",
    "    wrong = c3.actual != c3[column]\n",
    "    fn = c3[negative & wrong].shape[0]\n",
    "    \n",
    "    recalls.append(tp / (tp + fn))\n",
    "\n",
    "recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4efc5-96c3-4a73-8371-d79dc7b34a78",
   "metadata": {},
   "source": [
    "Model3 has the best performance in terms of recall so this would be the best model for our use case.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55402a-6a1c-4b14-8d7d-26ccaa4c03e8",
   "metadata": {},
   "source": [
    "- Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd27674-197e-4870-9cd1-a570f4d9cf86",
   "metadata": {},
   "source": [
    "In this case precision would be the appropriate metric to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e9ad2a-68ab-4268-ba6d-610230a9c607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.1, 0.13131313131313133]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = []\n",
    "columns = ['model1', 'model2', 'model3']\n",
    "\n",
    "for column in columns:\n",
    "    positive = c3[column] == 'Defect'\n",
    "    correct = c3.actual == c3[column]\n",
    "    tp = c3[positive & correct].shape[0]\n",
    "\n",
    "    wrong = c3.actual != c3[column]\n",
    "    fp = c3[positive & wrong].shape[0]\n",
    "    \n",
    "    recalls.append(tp / (tp + fp))\n",
    "\n",
    "recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b1f5f-a0cf-452f-9251-33503a3319c6",
   "metadata": {},
   "source": [
    "Model1 has the best performance in terms of precision for our use case.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6234e-b73f-4e8b-bd59-3896b99502ea",
   "metadata": {},
   "source": [
    "# 4\n",
    "\n",
    "You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812dca3a-fc62-47ce-8f4c-4a86438f85e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws = pd.read_csv('gives_you_paws.csv')\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93153a-3069-4bd2-8da0-8f34376ba15a",
   "metadata": {},
   "source": [
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "- In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd935e0b-ca9e-410a-a5c9-1d44063477da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a7bf5d-20b8-497c-8148-85bd8e6a3c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    5000\n",
       "Name: baseline, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws['baseline'] = 'dog'\n",
    "paws.baseline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ea9197-1bc0-4520-82db-6e7d5eae94ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8074, 0.6304, 0.5096, 0.7426, 0.6508]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "total = paws.shape[0]\n",
    "columns = paws.drop(columns = 'actual').columns\n",
    "\n",
    "for column in columns:\n",
    "    positive = paws[column] == 'dog'\n",
    "    correct = paws.actual == paws[column]\n",
    "    tp = paws[positive & correct].shape[0]\n",
    "    \n",
    "    negative = paws[column] == 'cat'\n",
    "    correct = paws.actual == paws[column]\n",
    "    tn = paws[negative & correct].shape[0]\n",
    "    \n",
    "    accuracies.append((tp + tn) / total)\n",
    "\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6436d80-ca88-4a17-b4e0-3b7db0c17cea",
   "metadata": {},
   "source": [
    "Models 1 and 4 are better than the baseline. Model 2 has similar performance to the baseline and model 3 is significantly worse than the baseline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27310c8a-bf04-462c-af89-13ca777721b9",
   "metadata": {},
   "source": [
    "- Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24030f28-7613-459b-a2e7-7f57adf0c889",
   "metadata": {},
   "source": [
    "Here we determine performance based on recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f29bb5-f03f-4dd4-806e-9ec93164558c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.803318992009834,\n",
       " 0.49078057775046097,\n",
       " 0.5086047940995697,\n",
       " 0.9557467732022127,\n",
       " 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = []\n",
    "\n",
    "for column in columns:\n",
    "    positive = paws[column] == 'dog'\n",
    "    correct = paws.actual == paws[column]\n",
    "    tp = paws[positive & correct].shape[0]\n",
    "    \n",
    "    negative = paws[column] == 'cat'\n",
    "    wrong = paws.actual != paws[column]\n",
    "    fn = paws[negative & wrong].shape[0]\n",
    "    \n",
    "    recalls.append(tp / (tp + fn))\n",
    "\n",
    "recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72051576-a572-4c16-810d-6fd640341699",
   "metadata": {},
   "source": [
    "Based on recall model 4 has the best performance so this would be best for phase I. For phase II we want better accuracy so model 1 is best.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27301b-0fab-4885-9325-dd6480f7ca3f",
   "metadata": {},
   "source": [
    "- Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812a417-2213-412e-8fe5-ec0740b14e29",
   "metadata": {},
   "source": [
    "Here we determine performance based on precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7c3587-b04e-47db-b0ca-dc59c0d82ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8900238338440586,\n",
       " 0.8931767337807607,\n",
       " 0.6598883572567783,\n",
       " 0.7312485304490948,\n",
       " 0.6508]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions = []\n",
    "\n",
    "for column in columns:\n",
    "    positive = paws[column] == 'dog'\n",
    "    correct = paws.actual == paws[column]\n",
    "    tp = paws[positive & correct].shape[0]\n",
    "    \n",
    "    wrong = paws.actual != paws[column]\n",
    "    fp = paws[positive & wrong].shape[0]\n",
    "    \n",
    "    precisions.append(tp / (tp + fp))\n",
    "\n",
    "precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95919e22-9a25-42b4-b9fa-d03853bb7e0c",
   "metadata": {},
   "source": [
    "Model 2 has the best performance in terms of precision so this is the best model to use for phase I. For phase II we want better accuracy so model 1 is best.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db72c3-e1ad-4baf-af1c-e45ec3400625",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "- sklearn.metrics.accuracy_score\n",
    "- sklearn.metrics.precision_score\n",
    "- sklearn.metrics.recall_score\n",
    "- sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc0620b-7568-499f-8eea-e114a23ca886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8074\n",
      "0.6304\n",
      "0.5096\n",
      "0.7426\n",
      "0.6508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(metrics.accuracy_score(paws.actual, paws[column])) for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e861a76-9e77-46aa-97d6-72daf9561ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8900238338440586\n",
      "0.8931767337807607\n",
      "0.6598883572567783\n",
      "0.7312485304490948\n",
      "0.6508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(metrics.precision_score(paws.actual, paws[column], pos_label = 'dog')) for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548d720e-8c1c-4036-b1b8-995fa734cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803318992009834\n",
      "0.49078057775046097\n",
      "0.5086047940995697\n",
      "0.9557467732022127\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(metrics.recall_score(paws.actual, paws[column], pos_label = 'dog')) for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "942c1bbf-fbd0-4e58-b9fa-17eeae99a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "model2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "model3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "model4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "baseline\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.00      0.00      0.00      1746\n",
      "         dog       0.65      1.00      0.79      3254\n",
      "\n",
      "    accuracy                           0.65      5000\n",
      "   macro avg       0.33      0.50      0.39      5000\n",
      "weighted avg       0.42      0.65      0.51      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in columns:\n",
    "    print(column)\n",
    "    print(metrics.classification_report(paws.actual, paws[column], zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
